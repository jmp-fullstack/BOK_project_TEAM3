{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## seedwords ##\n",
    "\n",
    "# doveish : []\n",
    "# hawkish : []\n",
    "\n",
    "# NBC에서 구한 단어 사전을 가지고 시작한다. \n",
    "\n",
    "# 중심단어 5 , 맥락단어5, 원도우크기5, 부정적샘플링 크기5, 벡터 차원30\n",
    "\n",
    "# NBC 모델을 사용하여 \"doveish\"와 \"hawkish\"에 해당하는 단어들을 추출\n",
    "# 추출된 단어들을 중심 단어와 맥락 단어로 사용하여 Word2Vec 모델을 학습하거나, 이미 학습된 Word2Vec 모델을 로드\n",
    "# Word2Vec 모델을 사용하여 각 단어에 대한 벡터를 얻어옵니다.\n",
    "# \"doveish\"와 \"hawkish\"에 해당하는 단어들의 벡터를 이용하여 유사한 단어들을 추출하고, 이를 seedwords로 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec 모델 로드\n",
    "word2vec_model = Word2Vec.load('your_word2vec_model_path')\n",
    "\n",
    "# 주어진 단어의 유사한 단어들을 추출하는 함수 정의\n",
    "def get_similar_words(word, topn=5):\n",
    "    # 주어진 단어와 가장 유사한 상위 topn개의 단어 추출\n",
    "    similar_words = word2vec_model.wv.most_similar(word, topn=topn)\n",
    "    # 추출된 단어들 반환\n",
    "    return [word for word, _ in similar_words]\n",
    "\n",
    "# NBC 모델을 사용하여 \"doveish\"와 \"hawkish\"에 해당하는 단어들 추출\n",
    "doveish_words = ['dove', 'peaceful', 'pacifist', 'conciliatory']  # NBC 모델을 통해 추출된 단어들로 가정\n",
    "hawkish_words = ['hawk', 'aggressive', 'militant', 'bellicose']  # NBC 모델을 통해 추출된 단어들로 가정\n",
    "\n",
    "# 주어진 단어들의 유사한 단어들을 추출하여 seedwords로 사용\n",
    "doveish_seedwords = [word for seedword in doveish_words for word in get_similar_words(seedword)]\n",
    "hawkish_seedwords = [word for seedword in hawkish_words for word in get_similar_words(seedword)]\n",
    "\n",
    "# seedwords 출력\n",
    "print(\"doveish seedwords:\", doveish_seedwords)\n",
    "print(\"hawkish seedwords:\", hawkish_seedwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 20:28:50,319 : INFO : collecting all words and their counts\n",
      "2024-03-26 20:28:50,319 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-03-26 20:28:50,320 : INFO : collected 11 word types from a corpus of 12 raw words and 4 sentences\n",
      "2024-03-26 20:28:50,320 : INFO : Creating a fresh vocabulary\n",
      "2024-03-26 20:28:50,320 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 11 unique words (100.00% of original 11, drops 0)', 'datetime': '2024-03-26T20:28:50.320118', 'gensim': '4.3.2', 'python': '3.11.8 | packaged by Anaconda, Inc. | (main, Feb 26 2024, 21:34:05) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2024-03-26 20:28:50,321 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 12 word corpus (100.00% of original 12, drops 0)', 'datetime': '2024-03-26T20:28:50.321116', 'gensim': '4.3.2', 'python': '3.11.8 | packaged by Anaconda, Inc. | (main, Feb 26 2024, 21:34:05) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2024-03-26 20:28:50,321 : INFO : deleting the raw counts dictionary of 11 items\n",
      "2024-03-26 20:28:50,321 : INFO : sample=0.001 downsamples 11 most-common words\n",
      "2024-03-26 20:28:50,322 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1.3823644488586286 word corpus (11.5%% of prior 12)', 'datetime': '2024-03-26T20:28:50.322113', 'gensim': '4.3.2', 'python': '3.11.8 | packaged by Anaconda, Inc. | (main, Feb 26 2024, 21:34:05) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2024-03-26 20:28:50,322 : INFO : estimated required memory for 11 words and 30 dimensions: 8140 bytes\n",
      "2024-03-26 20:28:50,322 : INFO : resetting layer weights\n",
      "2024-03-26 20:28:50,323 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-03-26T20:28:50.323111', 'gensim': '4.3.2', 'python': '3.11.8 | packaged by Anaconda, Inc. | (main, Feb 26 2024, 21:34:05) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "2024-03-26 20:28:50,323 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 11 vocabulary and 30 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-03-26T20:28:50.323111', 'gensim': '4.3.2', 'python': '3.11.8 | packaged by Anaconda, Inc. | (main, Feb 26 2024, 21:34:05) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2024-03-26 20:28:50,326 : INFO : EPOCH 0: training on 12 raw words (2 effective words) took 0.0s, 28169 effective words/s\n",
      "2024-03-26 20:28:50,328 : INFO : EPOCH 1: training on 12 raw words (2 effective words) took 0.0s, 6441 effective words/s\n",
      "2024-03-26 20:28:50,330 : INFO : EPOCH 2: training on 12 raw words (1 effective words) took 0.0s, 14641 effective words/s\n",
      "2024-03-26 20:28:50,333 : INFO : EPOCH 3: training on 12 raw words (1 effective words) took 0.0s, 3328 effective words/s\n",
      "2024-03-26 20:28:50,335 : INFO : EPOCH 4: training on 12 raw words (2 effective words) took 0.0s, 6952 effective words/s\n",
      "2024-03-26 20:28:50,336 : INFO : Word2Vec lifecycle event {'msg': 'training on 60 raw words (8 effective words) took 0.0s, 660 effective words/s', 'datetime': '2024-03-26T20:28:50.336076', 'gensim': '4.3.2', 'python': '3.11.8 | packaged by Anaconda, Inc. | (main, Feb 26 2024, 21:34:05) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2024-03-26 20:28:50,336 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=11, vector_size=30, alpha=0.025>', 'datetime': '2024-03-26T20:28:50.336076', 'gensim': '4.3.2', 'python': '3.11.8 | packaged by Anaconda, Inc. | (main, Feb 26 2024, 21:34:05) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation of 'this': [-0.00178742  0.0007881   0.01701117  0.03003091 -0.03100983 -0.0237227\n",
      "  0.02152958  0.02990996 -0.01671809 -0.01254457  0.02460168 -0.00511157\n",
      " -0.01512204  0.02184684 -0.01620053 -0.00605339  0.0095886   0.00330625\n",
      " -0.02761738 -0.03149606  0.02437255  0.01690087  0.02252564  0.00254289\n",
      "  0.02116963 -0.01135122 -0.00315467  0.01922858 -0.02507213 -0.01312034]\n",
      "Vector representation of 'sample': [ 1.33636435e-02  1.72845647e-02  1.41863385e-02  6.46584807e-03\n",
      " -1.05672078e-02  2.78461501e-02  3.20405997e-02  1.26420101e-02\n",
      " -9.45665035e-03  2.37584118e-05  4.06272803e-03 -2.81944163e-02\n",
      " -2.74131503e-02 -7.70052255e-04  4.12429171e-03 -1.91446021e-02\n",
      " -1.57509129e-02 -2.44869161e-02  2.77620517e-02  4.04326129e-04\n",
      " -1.50313294e-02  1.90056842e-02  3.06000505e-02 -1.36662405e-02\n",
      "  2.65489388e-02  1.79181136e-02  1.95970777e-02  1.70863466e-03\n",
      "  2.73769461e-02 -2.33968012e-02]\n",
      "Vector representation of 'sentence': [ 0.02605857 -0.03170062 -0.0006851   0.01156399 -0.00312991  0.02793924\n",
      "  0.03003594  0.02178835 -0.00237207  0.02570135 -0.02844778  0.01069036\n",
      " -0.01545999 -0.01696318  0.01196539  0.01790113  0.02589838 -0.01922169\n",
      "  0.02477787  0.02208499 -0.012366   -0.02915214  0.01812489  0.02169919\n",
      " -0.00262517 -0.02236619 -0.02361975 -0.00832353  0.01714418 -0.01221746]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 학습에 사용할 데이터 (2-gram 형식)\n",
    "sentences = [['this is', 'is a', 'a sample', 'sample sentence'],\n",
    "             ['this is', 'is another', 'another example', 'example sentence'],\n",
    "             ['yet another', 'another sentence'],\n",
    "             ['one more', 'more sentence']]\n",
    "\n",
    "# Word2Vec 모델 학습\n",
    "model = Word2Vec(sentences,\n",
    "                 sg=1,                  # Skip-gram 모델 사용 (CBOW 모델의 경우 sg=0)\n",
    "                 window=5,              # 원도우 크기 설정\n",
    "                 negative=5,            # 부정적 샘플링 크기 설정\n",
    "                 vector_size=30,        # 벡터 차원 설정\n",
    "                 min_count=1,           # 최소 단어 빈도 설정\n",
    "                 workers=4)             # 학습에 사용할 CPU 코어 수\n",
    "\n",
    "# 'this' 단어의 벡터 확인\n",
    "print(\"Vector representation of 'this':\", model.wv['this is'])\n",
    "\n",
    "# 'sample' 단어의 벡터 확인\n",
    "print(\"Vector representation of 'sample':\", model.wv['a sample'])\n",
    "\n",
    "# 'sentence' 단어의 벡터 확인\n",
    "print(\"Vector representation of 'sentence':\", model.wv['example sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w o r d   v e c\n"
     ]
    }
   ],
   "source": [
    "t = 'word vec'\n",
    "a = ' '.join(t)\n",
    "print(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
