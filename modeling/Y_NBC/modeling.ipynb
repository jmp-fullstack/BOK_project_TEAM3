{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import konlpy as kn\n",
    "import MeCab as mc\n",
    "import ekonlpy as ek\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pandas import Series, DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'words'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 크롤링 결과물 불러오기 - 이데일리 뉴스 (4분 소요)\n",
    "from ekonlpy.tag import Mecab\n",
    "from ekonlpy.sentiment import MPCK\n",
    "mpck = MPCK()\n",
    "\n",
    "# 패턴 정의\n",
    "news_pattern = ['\\\\[.*\\\\]','(?<=[가-힣])\\\\.','\\\\w{4,}\\\\@[a-zA-Z0-9\\\\-]{2,}\\\\.[a-z]{2,}(\\\\.[a-z]{2})?','....기자']\n",
    "bone_pattern = []\n",
    "\n",
    "result_path = './crawl_result/edaily_news_test.json'\n",
    "df = pd.read_json(result_path)\n",
    "df = df.sort_values(by=[\"date\"], ascending=False) # 날짜별 내림차순\n",
    "\n",
    "# 특정패턴 제거\n",
    "for pattern in news_pattern:\n",
    "    df['content'] = df['content'].apply(lambda x: re.sub(pattern, '', x))\n",
    "news1_df = df\n",
    "\n",
    "news1_df['words']=\" \"  # 빈 값을 가진 words column 추가\n",
    "for i, row in df.iterrows() :\n",
    "    try :\n",
    "        tmp_df = news1_df['content']\n",
    "        #임시 데이터프레임에 불러온 기사의 본문 내용 리스트로 저장\n",
    "        tokens = mpck.tokenize(tmp_df[i])\n",
    "        row['words'] = '.'.join(tokens)\n",
    "        df.loc[i, 'words'] = row['words']\n",
    "        #eKoNLPy로 tokenizing 한 것을 데이터프레임에 column 추가\n",
    "\n",
    "    except Exception as ex :\n",
    "        print(\"fail\")\n",
    "        \n",
    "news1_df = news1_df.drop(['content', 'title'], axis=1)  # axis=1은 열을 의미함\n",
    "news1_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'words'], dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 크롤링 결과물 불러오기 - 중앙일보 뉴스 (4분 소요)\n",
    "from ekonlpy.tag import Mecab\n",
    "from ekonlpy.sentiment import MPCK\n",
    "mpck = MPCK()\n",
    "\n",
    "# 패턴 정의\n",
    "news_pattern = ['\\\\[.*\\\\]','(?<=[가-힣])\\\\.','\\\\w{4,}\\\\@[a-zA-Z0-9\\\\-]{2,}\\\\.[a-z]{2,}(\\\\.[a-z]{2})?','....기자']\n",
    "bone_pattern = []\n",
    "\n",
    "result_path = './crawl_result/joongamg_news.json'\n",
    "df = pd.read_json(result_path)\n",
    "df = df.sort_values(by=[\"date\"], ascending=False) # 날짜별 내림차순\n",
    "\n",
    "# 특정패턴 제거\n",
    "for pattern in news_pattern:\n",
    "    df['content'] = df['content'].apply(lambda x: re.sub(pattern, '', x))\n",
    "news2_df = df\n",
    "\n",
    "news2_df['words']=\" \"  # 빈 값을 가진 words column 추가\n",
    "for i, row in df.iterrows() :\n",
    "    try :\n",
    "        tmp_df = news2_df['content']\n",
    "        #임시 데이터프레임에 불러온 기사의 본문 내용 리스트로 저장\n",
    "        tokens = mpck.tokenize(tmp_df[i])\n",
    "        row['words'] = '.'.join(tokens)\n",
    "        df.loc[i, 'words'] = row['words']\n",
    "        #eKoNLPy로 tokenizing 한 것을 데이터프레임에 column 추가\n",
    "\n",
    "    except Exception as ex :\n",
    "        print(\"fail\")\n",
    "        \n",
    "news2_df = news2_df.drop(['content', 'title'], axis=1)  # axis=1은 열을 의미함\n",
    "news2_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'words'], dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 크롤링 결과물 불러오기 - 머니투데이 뉴스 (4분 소요)\n",
    "from ekonlpy.tag import Mecab\n",
    "from ekonlpy.sentiment import MPCK\n",
    "mpck = MPCK()\n",
    "\n",
    "# 패턴 정의\n",
    "news_pattern = ['\\\\[.*\\\\]','(?<=[가-힣])\\\\.','\\\\w{4,}\\\\@[a-zA-Z0-9\\\\-]{2,}\\\\.[a-z]{2,}(\\\\.[a-z]{2})?','....기자']\n",
    "bone_pattern = []\n",
    "\n",
    "result_path = './crawl_result/moneytoday_news.json'\n",
    "df = pd.read_json(result_path)\n",
    "df = df.sort_values(by=[\"date\"], ascending=False) # 날짜별 내림차순\n",
    "\n",
    "# 특정패턴 제거\n",
    "for pattern in news_pattern:\n",
    "    df['content'] = df['content'].apply(lambda x: re.sub(pattern, '', x))\n",
    "news3_df = df\n",
    "\n",
    "news3_df['words']=\" \"  # 빈 값을 가진 words column 추가\n",
    "for i, row in df.iterrows() :\n",
    "    try :\n",
    "        tmp_df = news3_df['content']\n",
    "        #임시 데이터프레임에 불러온 기사의 본문 내용 리스트로 저장\n",
    "        tokens = mpck.tokenize(tmp_df[i])\n",
    "        row['words'] = '.'.join(tokens)\n",
    "        df.loc[i, 'words'] = row['words']\n",
    "        #eKoNLPy로 tokenizing 한 것을 데이터프레임에 column 추가\n",
    "\n",
    "    except Exception as ex :\n",
    "        print(\"fail\")\n",
    "        \n",
    "news3_df = news3_df.drop(['content', 'title'], axis=1)  # axis=1은 열을 의미함\n",
    "news3_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'words'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 크롤링 결과물 불러오기 - 머니투데이 뉴스 (4분 소요)\n",
    "from ekonlpy.tag import Mecab\n",
    "from ekonlpy.sentiment import MPCK\n",
    "mpck = MPCK()\n",
    "\n",
    "# 패턴 정의\n",
    "news_pattern = ['\\\\[.*\\\\]','(?<=[가-힣])\\\\.','\\\\w{4,}\\\\@[a-zA-Z0-9\\\\-]{2,}\\\\.[a-z]{2,}(\\\\.[a-z]{2})?','....기자']\n",
    "bone_pattern = []\n",
    "\n",
    "result_path = './crawl_result/minutes_re.tsv'\n",
    "df = pd.read_csv(result_path, sep='\\t')\n",
    "df = df.sort_values(by=[\"date\"], ascending=False) # 날짜별 내림차순\n",
    "\n",
    "# 특정패턴 제거\n",
    "for pattern in news_pattern:\n",
    "    df['content'] = df['content'].apply(lambda x: re.sub(pattern, '', x))\n",
    "min_df = df\n",
    "\n",
    "min_df['words']=\" \"  # 빈 값을 가진 words column 추가\n",
    "for i, row in df.iterrows() :\n",
    "    try :\n",
    "        tmp_df = min_df['content']\n",
    "        #임시 데이터프레임에 불러온 기사의 본문 내용 리스트로 저장\n",
    "        tokens = mpck.tokenize(tmp_df[i])\n",
    "        row['words'] = '.'.join(tokens)\n",
    "        df.loc[i, 'words'] = row['words']\n",
    "        #eKoNLPy로 tokenizing 한 것을 데이터프레임에 column 추가\n",
    "\n",
    "    except Exception as ex :\n",
    "        print(\"fail\")\n",
    "        \n",
    "min_df = min_df.drop(['content'], axis=1)  # axis=1은 열을 의미함\n",
    "min_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 결과물 불러오기 - 채권보고서 (4분 소요)\n",
    "from ekonlpy.tag import Mecab\n",
    "from ekonlpy.sentiment import MPCK\n",
    "mpck = MPCK()\n",
    "\n",
    "# 패턴 정의\n",
    "news_pattern = ['\\\\[.*\\\\]','(?<=[가-힣])\\\\.','\\\\w{4,}\\\\@[a-zA-Z0-9\\\\-]{2,}\\\\.[a-z]{2,}(\\\\.[a-z]{2})?','....기자']\n",
    "bone_pattern = []\n",
    "\n",
    "result_path = './crawl_result/bone_report.json'\n",
    "df = pd.read_json(result_path)\n",
    "df = df.sort_values(by=[\"date\"], ascending=False) # 날짜별 내림차순\n",
    "\n",
    "# 특정패턴 제거\n",
    "for pattern in news_pattern:\n",
    "    df['content'] = df['content'].apply(lambda x: re.sub(pattern, '', x))\n",
    "bone_df = df\n",
    "\n",
    "bone_df['words']=\" \"  # 빈 값을 가진 words column 추가\n",
    "for i, row in df.iterrows() :\n",
    "    try :\n",
    "        tmp_df = bone_df['content']\n",
    "        #임시 데이터프레임에 불러온 기사의 본문 내용 리스트로 저장\n",
    "        tokens = mpck.tokenize(tmp_df[i])\n",
    "        row['words'] = '.'.join(tokens)\n",
    "        df.loc[i, 'words'] = row['words']\n",
    "        #eKoNLPy로 tokenizing 한 것을 데이터프레임에 column 추가\n",
    "\n",
    "    except Exception as ex :\n",
    "        print(\"fail\")\n",
    "        \n",
    "bone_df = bone_df.drop(['content'], axis=1)  # axis=1은 열을 의미함\n",
    "bone_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 크롤링 데이터들 -> 데이터프레임으로 합치기\n",
    "crawl_df = pd.concat([news1_df, news2_df, news3_df, min_df, bone_df])\n",
    "crawl_df['date'] = pd.to_datetime(crawl_df['date']).dt.date\n",
    "crawl_df = crawl_df.dropna(subset=['date'])\n",
    "# crawl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_1 = pd.read_csv('./label_result/count_1gram.csv')\n",
    "count_2 = pd.read_csv('./label_result/count_2gram.csv')\n",
    "count_3 = pd.read_csv('./label_result/count_3gram.csv')\n",
    "count_4 = pd.read_csv('./label_result/count_4gram.csv')\n",
    "count_5 = pd.read_csv('./label_result/count_5gram.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([count_1, count_2, count_3, count_4, count_5], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df.set_index('ngram', inplace=True)\n",
    "\n",
    "# 열 추가\n",
    "k = 0.5\n",
    "df['P(word|P)'] = (df['P_count'] + k )/ (df['P_count'].sum() + 2 * k)\n",
    "df['P(word|N)'] = (df['N_count'] + k )/ (df['N_count'].sum() + 2 * k)\n",
    "df['log(P(word|P))'] = np.log(df['P(word|P)'])\n",
    "df['log(P(word|N))'] = np.log(df['P(word|N)'])\n",
    "\n",
    "# 폴라리티 스코어\n",
    "words = [index_value for index_value in df.index]\n",
    "\n",
    "# log 활용 underflow 방지\n",
    "final_P = np.log(df['P_count'].sum() / (df['P_count'].sum() + df['N_count'].sum()) )\n",
    "final_N = np.log(df['N_count'].sum() / (df['P_count'].sum() + df['N_count'].sum()) )\n",
    "scores = []\n",
    "for token in words:\n",
    "    final_P += df['log(P(word|P))'][token]\n",
    "    final_N += df['log(P(word|N))'][token]\n",
    "    final_P = np.exp(final_P)\n",
    "    final_N = np.exp(final_N)\n",
    "    a = final_P / (final_P + final_N)\n",
    "    b = final_N / (final_P + final_N)\n",
    "    scores.append(a / b)\n",
    "    # final_P = np.log(df['P_count'].sum() / (df['P_count'].sum() + df['N_count'].sum()) )\n",
    "    # final_N = np.log(df['N_count'].sum() / (df['P_count'].sum() + df['N_count'].sum()) )\n",
    "df['Polarity'] = scores\n",
    "\n",
    "P_seed_words = []\n",
    "N_seed_words = []\n",
    "# P_seed_pola = []\n",
    "# N_seed_pola = []\n",
    "for word, pola in zip(df.index, df['Polarity']):\n",
    "    if pola > 1.3 :\n",
    "        P_seed_words.append(word)\n",
    "        # P_seed_pola.append(pola)\n",
    "    elif pola < 0.7 :\n",
    "        N_seed_words.append(word)\n",
    "        # N_seed_pola.append(pola)\n",
    "    else :\n",
    "        pass\n",
    "\n",
    "# P_seed = list(zip(P_seed_words, P_seed_pola))\n",
    "# N_seed = list(zip(N_seed_words, N_seed_pola))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos, neg, mid 변수를 초기화합니다.\n",
    "pos_dict = {}\n",
    "neg_dict = {}\n",
    "mid_dict = {}\n",
    "dict_tone = {}\n",
    "\n",
    "# 각 날짜별로 데이터를 처리합니다.\n",
    "for date, tokens in zip(min_df['date'], min_df['words']):\n",
    "    # 날짜별로 pos, neg, mid 변수 초기화\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    mid = 0\n",
    "    # 해당 날짜의 긍정적인 단어와 부정적인 단어를 추출합니다.\n",
    "    # P_seed_words = get_positive_words(date)  # 날짜에 따라 긍정적인 단어를 가져오는 함수를 정의해야 합니다.\n",
    "    # N_seed_words = get_negative_words(date)  # 날짜에 따라 부정적인 단어를 가져오는 함수를 정의해야 합니다.\n",
    "    # 문장을 마침표를 기준으로 분할하여 개별 문장들을 추출합니다.\n",
    "    sents = tokens.split('.')\n",
    "    # 각 문장에 대해 긍정적인 단어와 부정적인 단어가 포함되어 있는지 확인하고, 해당하는 문장을 카운트합니다.\n",
    "    for sent in sents:\n",
    "        if any(word in sent for word in P_seed_words):\n",
    "            pos += 1\n",
    "        elif any(word in sent for word in N_seed_words):\n",
    "            neg += 1\n",
    "        else:\n",
    "            mid += 1\n",
    "    # 결과를 해당 날짜의 딕셔너리에 저장합니다.\n",
    "    pos_dict[date] = pos\n",
    "    neg_dict[date] = neg\n",
    "    mid_dict[date] = mid\n",
    "    \n",
    "    # 톤을 저장합니다.\n",
    "    tones = ((pos - neg) + 0.00001) / ((pos + neg) + 0.00001)\n",
    "    if tones == 0 :\n",
    "            pass\n",
    "    else :\n",
    "        if date not in dict_tone:\n",
    "            dict_tone[date] = [tones]\n",
    "        else:\n",
    "            dict_tone[date].append(tones)\n",
    "\n",
    "# dict_tone\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "average_tone_date = {}\n",
    "for date, tones in dict_tone.items():\n",
    "    average_tone = np.mean(tones)\n",
    "    average_tone_date[date] = average_tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 딕셔너리를 DataFrame으로 변환\n",
    "df = pd.DataFrame(list(average_tone_date.items()), columns=['Date', 'Tone'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "df.to_csv(\"min_tone_final.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
